{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 16\n",
    "lora_alpha = 16\n",
    "random_state = 3407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = r,\n",
    "    lora_alpha = lora_alpha,\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = random_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "dataset:  Dataset  = load_dataset(\"beautyyuyanli/deep-sink-data\", split = \"train\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "template =Template(\"\"\"\n",
    "<｜User｜>写一篇爆仓文学，主题：{{topic}}\n",
    "<｜Assistant｜><think>\\n{{think}}\\n</think>\\n\\n{{content}}\\n<｜end▁of▁sentence｜>\n",
    "\"\"\".strip())\n",
    "def map_fn(e):\n",
    "    return {\"text\": template.render(topic=e[\"topic\"], think=e[\"think\"], content=e[\"content\"])}\n",
    "    \n",
    "dt_text = dataset.map(map_fn, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dt_text,\n",
    "    dataset_text_field = \"text\",\n",
    "    # max_seq_length = max_seq_length,\n",
    "    # dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # max_steps = 60,\n",
    "        num_train_epochs = 10, # For longer training runs!\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = random_state,\n",
    "        output_dir = \"../../../output_model\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import unsloth_train\n",
    "unsloth_train(trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanli.yu/DeepSink/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.1.8: Fast Qwen2 patching. Transformers: 4.48.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.684 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.21it/s]\n",
      "Unsloth 2025.1.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"../../../output_model/checkpoint-140\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "test = \"\"\"\n",
    "<｜User｜>写一篇爆仓文学。不得夹杂英文。\n",
    "<｜Assistant｜><think>\n",
    "\"\"\".strip()\n",
    "input_ids = tokenizer(\n",
    "    test,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "input_ids = input_ids[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我应该这样设计写作思路:首先,我选择一个在金融市场爆仓的事件作为故事的核心冲突,这能迅速抓住读者的注意力,并营造紧张的氛围。为了增强故事的真实感和代入感,我使用了精确的时间点(凌晨3:17)和具体的数字(200万保证金、杠杆倍数、账户余额)。故事中嵌入了金融行业的专业术语(CTO、杠杆、保证金、强平、流动性、合约、杠杆体系),这不仅提升了故事的专业性,也让主人公成为了故事的“试验品”,让读者更容易产生共鸣。\n",
      "\n",
      "为了更有效地表达主人公的内心世界,我使用了第一人称叙述,并细腻地描写了他的身体反应(喉咙发紧、手抖、汗毛竖起)以及他的外部环境(凌晨三点、电梯里、咖啡店、手机没电),以此来增强故事的感染力。我设计了多个原因导致爆仓(市场崩盘、公司重组、杠杆体系错误),这反映了金融市场的复杂性和不可预测性,也突出了主人公的盲目自信和错误判断。结尾处,主人公还在咖啡店等待dehydration报警,这暗示了他可能面临更严重的后果,也留下了余韵,让读者回味思考。\n",
      "\n",
      "为了提升故事的层次感,我穿插了主人公的回忆(三年前在杭州打工、买股票、> 开始积累经验和心理准备),并与外部事件(金融公司重组、政策变化)形成了呼应。这种前后对比,不仅强化了主题,也展现了主人公的悲剧性命运。我使用了对比手法,将主人公之前的贪婪与后来的绝望形成对比,将他与之前在杭州打工的朋友形成对比,将他与那些了解期权知识的“专家”形成对比,以此来突出个人在群体面前的渺小和最终的失败。\n",
      "\n",
      "在情感表达上,我力求细致而真实。主人公从希望到焦虑再到绝望,我细致地描绘了他的身体反应和心理活动,例如,喉咙发紧、手抖、汗毛竖起等细节,力求让读者感受到他的痛苦和无奈。我设置了一些伏笔(朋友的谎言、市场崩盘、政策变化),来进一步加剧故事的悲剧色彩。结尾处,主人公等待dehydration报警,这不仅是一个生理层面的描绘,更是一种对金融风险不可抗拒性的隐喻。\n",
      "\n",
      "我使用了象征手法,“dehydration报警”可以解读为一种无法摆脱的困境,“强平提示”则象征着金融体系对个体的控制。我运用了时间(凌晨三点、> )、具体的数字(200万、500元余额)以及专业的金融术语(期权、> 、杠杆体系错误),这些元素共同构建了故事的紧张和专业性。\n",
      "\n",
      "总而言之,我力图通过紧凑的情节、细致的描写和多样的写作手法,刻画一个在金融市场中孤独、绝望和自我毁灭的故事,并以此来引发读者对投机风险的反思。\n",
      "</think>\n",
      "\n",
      "凌晨三点三十七分,电梯里挤得人多。手机电量已降得只剩待机模式,强平通知的红色弹窗还在 continuously 击打视网膜。窗外的深圳楼群在夜光灯下变得模糊,但我知道——那不是最后的光。\n",
      "\n",
      "三年前在杭州打工时的我,都没想到会在中文金融群里学会用杠杆做空。当时同事推荐了个套利指标,说当深成Index突破20000点时,用20倍杠杆从深圳期交所借资金,三天绝对吃透成本。我把攒下的30万积蓄全换成现货保证金,加上辞职前最后五天刷夜班攒的1800块工时费,总共200万。\n",
      "\n",
      "前两周确实顺利,指标显示的形态都能完美匹配我的仓位。深圳每小时2%的盘口切换速度,让我在早盘吃透点利后,直接在下午三点把仓位清零。直到上周四,当某国际资本突然宣布收购深圳Midtown基金时,所有指标都变成垂直竖线。\n",
      "\n",
      "我在凌晨一点五十七分打开合约账户时,保证金比例已经从72%骤降到15%。这时候我刚从回笼的最后20000块转出500元备用金,屏幕上的红色数字像极了《期权定价理论》里那些公式里跳动的负值。\n",
      "\n",
      "其实公司重组的公告是三小时前投到网上的,但没人愿意承认错。直到政策局发布《关于抑制投机行为的通知》的时候,我才敢打开交易客户端。五点四十七分,价格开始垂直下跌——但指标显示的支撑位比实际成交价低15%。\n",
      "\n",
      "最后的强平发生在七点三十七分。系统自动清除了所有持仓,还倒着补了三笔补仓请求。手机在抽屉里翻到那张写满 advised prices 的草稿纸,突然想起那个给我发年化20000元推荐费的朋友,他上周还说深圳期交所上线时的流动性缺口足够让整个团队套现。\n",
      "\n",
      "现在地铁口的自动贩卖机在响,我盯着空缺的咖啡杯看发呆。朋友上周给我的聊天记录里,有张他母亲的身份证复印件——五年前他贷款买房时用的200万贷款合同,现在看来和现在账户里的强平通知,根本就是同一张纸的不同面。\n",
      "\n",
      "凌晨三点的电梯里,有个穿红马甲的中介递过来纸巾:“您看这个月的交易记录,有赚到的吗?”我攥着纸巾的手在发抖,屏幕上闪烁的数字像在嘲笑我。而那个在金融公司当CTO的同学,三天前已经把杠杆体系的错误发给了我——他用5倍杠杆做多深圳指数,实际持仓成本比现价低40%。\n",
      "<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 4096, pad_token_id = tokenizer.eos_token_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
